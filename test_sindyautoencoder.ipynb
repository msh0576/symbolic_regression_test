{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선, SINDy-autoencoder 학습 코딩 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sihoon/works/HAVOK/sindy_test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sys\n",
    "import pyreadr\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "source_path = '/home/sihoon/works/HAVOK/SindyPendulum_main/src'\n",
    "if source_path not in sys.path:\n",
    "    sys.path.insert(0, source_path)\n",
    "from sindy_library import SINDyLibrary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation - pendulum image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pend(y, t,):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, - np.sin(theta)]\n",
    "    return dydt\n",
    "\n",
    "def pend_damp(y, t,):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, - np.sin(theta)-0.1*omega]\n",
    "    return dydt\n",
    "\n",
    "def select_ics(theta0,omegaics0):\n",
    "    ics = []\n",
    "    for i in range(n_ics):\n",
    "        for j in range(n_ics):\n",
    "            lim = (np.abs((omega0[j]**2)/2 - np.cos(theta0[i])))\n",
    "            if lim <  0.99 :\n",
    "                ics.append((theta0[i],omega0[j]))\n",
    "    return ics\n",
    "\n",
    "def wrap_to_pi(z):\n",
    "    z_mod = z % (2*np.pi)\n",
    "    subtract_m = (z_mod > np.pi) * (-2*np.pi)\n",
    "    return z_mod + subtract_m\n",
    "\n",
    "def image_gen(ics):\n",
    "    x = np.linspace(-1.5, 1.5, NX)\n",
    "    y = np.linspace(-1.5, 1.5, NY)\n",
    "    xx,yy = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "    data = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n",
    "    data2 = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n",
    "\n",
    "\n",
    "    for idx in range(len(ics)):\n",
    "        if(idx%100==0): print(idx,' su ', len(ics))\n",
    "        y0 = [ics[idx][0], ics[idx][1]]\n",
    "        sol = odeint(pend, y0,t)\n",
    "        theta = sol[:,0]\n",
    "        omega = sol[:,1]\n",
    "\n",
    "        temp = []\n",
    "        for i in range(len(theta)):\n",
    "            z = np.exp(- 20 *((xx - np.cos(theta[i] + np.pi/2))*(xx - \n",
    "                np.cos(theta[i] +np.pi/2))) - 20 * ((yy -np.sin(theta[i]+np.pi/2))*(yy -np.sin(theta[i]+np.pi/2))))\n",
    "            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n",
    "    \n",
    "            temp.append(z)\n",
    "        data[idx] = np.array(temp)\n",
    "        \n",
    "        temp = []\n",
    "        for i in range(len(omega)):            \n",
    "            exp = -20 * 2 * omega[i]*(np.cos(theta[i]+ np.pi/2) - np.sin(theta[i] + np.pi/2))\n",
    "            z = np.exp(- 20 *((xx - np.cos(theta[i] + np.pi/2))*(xx - \n",
    "                np.cos(theta[i] +np.pi/2))) - 20 * ((yy -np.sin(theta[i]+np.pi/2))*(yy -np.sin(theta[i]+np.pi/2))))\n",
    "            z = z * exp\n",
    "            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n",
    "            temp.append(z)\n",
    "        data2[idx] = np.array(temp)\n",
    "        \n",
    "    return data,data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  su  1432\n",
      "100  su  1432\n",
      "200  su  1432\n",
      "300  su  1432\n",
      "400  su  1432\n",
      "500  su  1432\n",
      "600  su  1432\n",
      "700  su  1432\n",
      "800  su  1432\n",
      "900  su  1432\n",
      "1000  su  1432\n",
      "1100  su  1432\n",
      "1200  su  1432\n",
      "1300  su  1432\n",
      "1400  su  1432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(143200, 2601)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ics = 50\n",
    "    \n",
    "#COSTANTI e PARAMETRI\n",
    "ta = 0.\n",
    "tb = 5\n",
    "dt = 0.05\n",
    "# tb = 10\n",
    "# dt = 0.1\n",
    "NX = 51\n",
    "NY = 51\n",
    "\n",
    "\n",
    "t = np.arange(ta, tb ,dt)   # (100,)\n",
    "theta0 = np.linspace(-np.pi,np.pi,n_ics)\n",
    "omega0 = np.linspace(-2.1, 2.1,n_ics)\n",
    "\n",
    "ics = select_ics(theta0,omega0)\n",
    "data,data2 = image_gen(ics)\n",
    "\n",
    "\n",
    "#questo reshape serve per mandare al autoencoder delle immagini flat\n",
    "#TODO verifica che sia corretto questo rehsape --> dovrebb essere ok fatto prova su colab\n",
    "X = data.reshape((len(ics) * len(t),NX * NY))   # (1432 (ics size?) * 100 (t size), 2602 (image size))\n",
    "Xdot = data2.reshape((len(ics) * len(t),NX * NY))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data - TE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_path = './Dataset/TEP_FaultFree_Training.RData'\n",
    "train_faulty_path = './Dataset/TEP_Faulty_Training.RData'\n",
    "\n",
    "test_normal_path = './Dataset/TEP_FaultFree_Testing.RData'\n",
    "test_faulty_path = './Dataset/TEP_Faulty_Testing.RData'\n",
    "\n",
    "\n",
    "train_df = pyreadr.read_r(train_normal_path)['fault_free_training']    # (250000, 55)\n",
    "test_df = pyreadr.read_r(test_faulty_path)['faulty_testing']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = train_df.columns[3:44]\n",
    "train_normal_df = train_df[(train_df.faultNumber==0)&(train_df.simulationRun.isin(range(2)))][feature_names]   # (500, 52)\n",
    "X_np = train_normal_df.to_numpy()\n",
    "X = X_np[:, :]\n",
    "# Xdot = X_np[1:, :]\n",
    "\n",
    "\n",
    "\n",
    "dt = 0.1\n",
    "n_states = len(train_normal_df.columns)\n",
    "t_max = len(train_normal_df.index)\n",
    "t = np.arange(1, t_max+1, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_increasing_by_dt(size, dt=0.1):\n",
    "    output_list = []\n",
    "    current_value = 0.1\n",
    "\n",
    "    for _ in range(size):\n",
    "        output_list.append(current_value)\n",
    "        current_value += dt\n",
    "\n",
    "    return np.array(output_list)\n",
    "\n",
    "t = generate_list_increasing_by_dt(size=t_max, dt=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate dxdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler.fit_transform(X = train_normal_df))\n",
    "train_df_scaled\n",
    "# X_np = train_df_scaled.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).float().to(device)\n",
    "Xdot = torch.from_numpy(Xdot).float().to(device)\n",
    "X.shape, Xdot.shape, X.dtype\n",
    "\n",
    "val_size = round(X.shape[0] * 0.1)  # 10% of train size\n",
    "train_size = X.shape[0] - val_size\n",
    "train_size, val_size\n",
    "\n",
    "batch_size = 8192\n",
    "my_dataset = TensorDataset(X,Xdot)\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(my_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\n",
    "val_loader   = DataLoader(val_subset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,latent_dim)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            print(m)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #nn.init.xavier_normal_(m.weight) #per la sigmoid\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))   \n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size,latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,128)\n",
    "        self.fc4 = nn.Linear(128,input_size)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))   \n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_size,latent_dim)\n",
    "        self.decoder = Decoder(input_size,latent_dim)\n",
    "        self.SINDyLibrary = SINDyLibrary(\n",
    "            device=device,\n",
    "            latent_dim=latent_dim,\n",
    "            include_biases=False,\n",
    "            include_states=True,\n",
    "            include_sin=True,\n",
    "            include_cos=False,\n",
    "            include_multiply_pairs=False, #non ho capito cosa é ma conta due volte le coppie  \n",
    "            poly_order=1,\n",
    "            include_sqrt=False,\n",
    "            include_inverse=False,\n",
    "            include_sign_sqrt_of_diff=False)\n",
    "        \n",
    "\n",
    "        self.XI = nn.Parameter(torch.full((self.SINDyLibrary.number_candidate_functions,latent_dim),1.,dtype = torch.float32,requires_grad=True,device = device))\n",
    "#        self.XI = nn.Parameter(torch.randn((self.SINDyLibrary.number_candidate_functions,latent_dim),\n",
    "        self.XI_coefficient_mask = torch.ones((self.SINDyLibrary.number_candidate_functions,latent_dim),dtype = torch.float32, device=device)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        learning_rate = 1e-4\n",
    "        return torch.optim.SGD(self.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "        #return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def t_derivative(self,input, xdot, weights, biases, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Compute the first order time derivatives by propagating through the network.\n",
    "        da[l]dt = xdot * da[l]dx = xdot * product(g'(w[l]a[l-1] + b[l])* w[l])\n",
    "        Arguments:\n",
    "            input - 2D tensorflow array, input to the network. Dimensions are number of time points\n",
    "            by number of state variables.\n",
    "            xdot - First order time derivatives of the input to the network. quello che conosciamo\n",
    "            weights - List of tensorflow arrays containing the network weights\n",
    "            biases - List of tensorflow arrays containing the network biases\n",
    "            activation - String specifying which activation function to use. Options are\n",
    "            'elu' (exponential linear unit), 'relu' (rectified linear unit), 'sigmoid',\n",
    "            or linear.\n",
    "\n",
    "        Returns:\n",
    "            dadt - Tensorflow array, first order time derivatives of the network output.\n",
    "        \"\"\"\n",
    "        a   = input\n",
    "        dadt = xdot #per le condizioni iniziali\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            for i in range(len(weights) - 1):\n",
    "                z = torch.matmul(a, weights[i].T) + biases[i]\n",
    "                a = torch.sigmoid(z)\n",
    "                gprime = a * (1-a)\n",
    "                dadt = gprime * torch.matmul(dadt, weights[i].T)\n",
    "            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n",
    "            \n",
    "        elif activation == 'relu':\n",
    "            for i in range(len(weights) - 1):\n",
    "                z = torch.matmul(a, weights[i].T) + biases[i]\n",
    "                a = torch.relu(z)\n",
    "                dadt = (z > 0).float() * torch.matmul(dadt, weights[i].T)    \n",
    "            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n",
    "        return dadt #nel caso che ci serve dadt sará l output dell encoder ossia le latent variables!\n",
    "\n",
    "    \n",
    "    \n",
    "    def compute_quantities(self,x,xdot):\n",
    "    \n",
    "        z = self.encoder(x)\n",
    "        xtilde = self.decoder(z)\n",
    "\n",
    "        theta = self.SINDyLibrary.transform(z) \n",
    "        zdot_hat = torch.matmul(theta, self.XI_coefficient_mask * self.XI)\n",
    "        \n",
    "        encoder_parameters = list(self.encoder.parameters())\n",
    "        encoder_weight_list = [w for w in encoder_parameters if len(w.shape) == 2]\n",
    "        encoder_biases_list = [b for b in encoder_parameters if len(b.shape) == 1]\n",
    "        zdot = self.t_derivative(x, xdot, encoder_weight_list, encoder_biases_list, activation='relu')                                               \n",
    "\n",
    "        #print(\"propagazione sul decoder\")\n",
    "        decoder_parameters = list(self.decoder.parameters())\n",
    "        decoder_weight_list = [w for w in decoder_parameters if len(w.shape) == 2]\n",
    "        decoder_biases_list = [b for b in decoder_parameters if len(b.shape) == 1]\n",
    "        xtildedot = self.t_derivative(z, zdot_hat, decoder_weight_list, decoder_biases_list, activation='relu')    \n",
    "        \n",
    "        return xtilde, xtildedot, z, zdot, zdot_hat\n",
    "\n",
    "    def loss_function(self, x, xdot, xtilde, xtildedot, zdot, zdot_hat,XI):\n",
    "        mse = nn.MSELoss()\n",
    "        alpha1 = 5e-4\n",
    "        alpha2 = 5e-5\n",
    "        alpha3 = 1e-5\n",
    "\n",
    "\n",
    "        loss = {}\n",
    "        loss['recon_loss'] = mse(x, xtilde) #errore di ricostruzione \n",
    "        loss ['sindy_loss_x'] = mse(xdot, xtildedot) \n",
    "        loss ['sindy_loss_z'] = mse(zdot, zdot_hat) \n",
    "        loss['sindy_regular_loss'] = torch.sum(torch.abs(XI)) #norma L1 degli XI\n",
    "        loss['tot'] = loss['recon_loss'] + alpha1*loss['sindy_loss_x'] + alpha2*loss['sindy_loss_z'] + alpha3*loss['sindy_regular_loss']\n",
    "        tot = loss['tot']\n",
    "        return tot, loss\n",
    "    \n",
    "    def forward(self, x, xdot):\n",
    "        return self.compute_quantities(x, xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (fc1): Linear(in_features=41, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=41, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "input_size = X.shape[1]\n",
    "latent_dim = 10\n",
    "\n",
    "model = Autoencoder(input_size,latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_threshold(t):\n",
    "    if (t % seq_thres == 0 and t>1):\n",
    "        model.XI_coefficient_mask = torch.abs(model.XI) > 0.1\n",
    "\n",
    "def save_model(t):\n",
    "    if SAVE == True:\n",
    "        if t % saving_rate == 0 and t > 0:\n",
    "            print('salvataggio a ',t,\"epoche\")\n",
    "            f1 = path + 'model' + '_' + str(t) + 'epochs' + '.pt'\n",
    "            torch.save({\n",
    "            'epoch': t,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': model.configure_optimizers().state_dict(),\n",
    "            'loss': model.loss_function,\n",
    "            'sindy_coefficients': model.XI,\n",
    "            'coefficient_mask' : model.XI_coefficient_mask\n",
    "            }, f1)\n",
    "            \n",
    "            XI = model.XI.cpu().detach().numpy()\n",
    "            np.save(path + 'model' + '_' + str(t) + 'epochs' + '.npy',XI)\n",
    "\n",
    "def print_model(t):\n",
    "    f = open(path + 'model_equation' + '_' + str(t) + '.txt', 'w')\n",
    "    coefficient_mask = model.XI_coefficient_mask.cpu().detach().numpy()\n",
    "    XI = model.XI.cpu().detach().numpy()\n",
    "    feature_list = model.SINDyLibrary.get_feature_names()\n",
    "    for j in range(latent_dim):\n",
    "        for i in range(len(feature_list)-1):\n",
    "            coeff = XI[i][j] * coefficient_mask[i][j]\n",
    "            if  abs(coeff) >= 0.1 and coeff != 0:\n",
    "                #print(f\"dz{j} = {coeff:.4f} {feature_list[i]}\")\n",
    "                f.write(f\"dz{j} = {coeff:.6f} {feature_list[i]}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training step for one epoch\n",
    "def train_step(loss_list):\n",
    "        \n",
    "    #queste serve per le loss di ogni batch\n",
    "    loss_epoch = {}\n",
    "    loss_epoch['recon_loss'] = []\n",
    "    loss_epoch['sindy_loss_x'] = []\n",
    "    loss_epoch['sindy_loss_z'] = []\n",
    "    loss_epoch['sindy_regular_loss'] = []\n",
    "    loss_epoch['tot'] = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X,Xdot) in enumerate (train_loader):\n",
    "        X.to(device) #per passarlo alla gpu\n",
    "        Xdot.to(device)\n",
    "        #forward pass \n",
    "        xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer = model.configure_optimizers()\n",
    "        loss, loss_dict = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for key in loss_epoch.keys():\n",
    "            loss_epoch[key].append(loss_dict[key].item())\n",
    "        \n",
    "        \n",
    "    for key in loss_epoch.keys():\n",
    "            loss_list[key].append(sum(loss_epoch[key])/len(loss_epoch[key]))\n",
    "            \n",
    "    del loss_epoch,loss_dict,xtilde, xtildedot, z, zdot, zdot_hat,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(loss_val_list):\n",
    "    loss_val_epoch = []\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    for batch, (X,Xdot) in enumerate (val_loader):\n",
    "        X.to(device) #per passarlo alla gpu\n",
    "        Xdot.to(device)\n",
    "        #forward pass \n",
    "        xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n",
    "        # validation loss \n",
    "        loss, _ = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n",
    "        loss_val_epoch.append(loss.item())\n",
    "        del  xtilde, xtildedot, z, zdot, zdot_hat,loss\n",
    "\n",
    "    loss_val_list.append(sum(loss_val_epoch)/len(loss_val_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(t):\n",
    "    if t % loss_rate == 0:\n",
    "        print()\n",
    "        print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        for key in loss_list.keys():\n",
    "            temp = loss_list[key]\n",
    "            print(f'{key} of epoch {t}: {temp[-1]:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "\n",
    "VALIDATION = False\n",
    "SAVE = True\n",
    "\n",
    "epochs = 2000\n",
    "path = './sindy_autoencoder_model/'\n",
    "\n",
    "seq_thres = 100   # 에포크 수, 얼마 간격으로 seq thres 수행\n",
    "\n",
    "equation_rate = 100 # 찾은 방정식 출력\n",
    "\n",
    "saving_rate = 200 # numero epoche in cui salvo il modello e gli XI\n",
    "\n",
    "loss_rate = 50  #print delle loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = {}\n",
    "loss_list['recon_loss'] = []\n",
    "loss_list['sindy_loss_x'] = []\n",
    "loss_list['sindy_loss_z'] = []\n",
    "loss_list['sindy_regular_loss'] = []\n",
    "loss_list['tot'] = []\n",
    "loss_val_list = []\n",
    "\n",
    "\n",
    "while t <= epochs:\n",
    "    print(\"epoch:\",t)\n",
    "    train_step(loss_list)\n",
    "    \n",
    "    validation_step(loss_val_list)\n",
    "    \n",
    "    print_loss(t)\n",
    "\n",
    "    save_model(t)\n",
    "    \n",
    "    sequential_threshold(t)\n",
    "    \n",
    "    if t % equation_rate == 0 and t>1:\n",
    "        print_model(t)\n",
    "        data = pd.DataFrame.from_dict(loss_list)\n",
    "        data.to_csv(path + 'data_'+str(t)+'epochs'+'.csv')\n",
    "    t = t + 1\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0691a1fc411f7154f05588b3487ba90d4557b49646e121b81e68b74191c18961"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('SINDy_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
